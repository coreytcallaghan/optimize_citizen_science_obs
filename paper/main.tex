\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasresearcharticle} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Optimizing the future of biodiversity sampling by citizen scientists}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,b,1]{Corey T. Callaghan}
\author[c,]{Alistair G. B. Poore}
\author[b,a]{Richard E. Major}
\author[b,a]{Jodi J. L. Rowley}
\author[a,c]{William K. Cornwell}

\affil[a]{Centre for Ecosystem Science, School of Biological, Earth and Environmental Sciences, UNSW Sydney, Sydney, 2052, NSW, Australia}
\affil[b]{Australian Museum Research Institute, Australian Museum, Sydney, 2000, NSW, Australia}
\affil[c]{Ecology and Evolution Research Centre, School of Biological, Earth and Environmental Sciences, UNSW Sydney, Sydney, 2052, NSW, Australia}

% Please give the surname of the lead author for the running footer
\leadauthor{Callaghan}

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Citizen scientists are increasingly monitoring biodiversity, but we need to better understand how to optimally sample biodiversity in time and space for intended outcomes, relying on citizen science data.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Author contributions: C.T.C., W.K.C. designed research, performed research, and analyzed research. C.T.C., W.K.C., J.J.L.R., A.G.B.P., and R.E.M. wrote the paper.}
\authordeclaration{The authors declare no conflict of interest.}

\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: c.callaghan@unsw.edu.au}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{biodiversity $|$ conservation $|$ citizen science $|$ optimal sampling $|$ eBird}

\begin{abstract}
Citizen science is so so important. yata yata yata
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{A}ssessing biodiversity trends in space and time is essential for conservation \cite{harrison2014assessing, wilson2011modelling, mcmahon2011improving, honrado2016fostering} \parshape=0. Reliable biodiversity estimates allow us to track our global progress in curbing biodiversity loss \cite{harrison2014assessing}. Unsurprisingly, reliable trend detection is usually derived from well-designed surveys in space and time \cite{harrison2014assessing, vellend2017estimates, kery2009trend}, that are long-term \cite{lindenmayer2012value, magurran2010long}. But scientific funding for long-term ecological and conservation research is rapidly disappearing [REFS???]. Increasingly, government agencies, scientific researchers, and conservationists are turning to citizen science data to help inform the state of biodiversity at local \cite{callaghan2015efficacy, theobald2015global, sullivan2017using, loss2015linking}, regional \cite{barlow2015citizen, fox2011new}, and global scales \cite{chandler2017contribution, pocock2018vision, cooper2014invisible}.

Citizen science --- the cooperation between a range of experts and non-experts  --- is an incredibly diverse field \cite{jordan2015citizen}. Projects generally fall along a continuum based on the level of associated structure \cite{kelling2019using, welvaert2016citizen}, ranging from unstructured (e.g., iNaturalist) to structured (U.K. Butterfly Monitoring Scheme). The level of structure, in turn, influences the biases and data quality concerns of a particular project. For instance, observer skill \cite{kelling2015can}, time-of-day, number of participants in a group, and technological capabilities of a participant may influence the data collected by some, but not necessarily all, citizen science projects. Generalizeable among citizen science projects, however, are various spatial and temporal biases \cite{boakes2010distorted, bird2014statistical}. Observers submitting observations on weekends \cite{courter2013weekend}, sampling near roads and human settlements \cite{kelling2015taking}, and observers visiting known 'hotspots' for biodiversity are all examples of spatial and temporal biases. Importantly, these biases are not restricted to citizen science projects. 'Professional scientists' also have biased sampling of biodiversity, highlighted by biased museum collections [REFS???].

Fortunately, in the case of broad-scale biodiversity data collected by citizen scientists, these various biases can generally be accounted for, statistically \cite{isaac2014statistics, robinson2018correcting}, by filtering or subsetting the data \cite{wiggins2011conservation}, pooling multiple data sources \cite{fithian2015bias}, or machine learning and hierarchical clustering techniques \cite{hochachka2012data, kelling2015taking}. Despite these biases, citizen science data have increased our knowledge on species distribution models \cite{bradsworth2017species, van2013opportunistic}, niche breadth \cite{tiago2017using}, biodiversity measurements \cite{stuart2017assessing, pocock2018vision}, phenological research \cite{la2014role, supp2015citizen}, invasive species detection \cite{pocock2017citizen, grason2018citizen}, and phylogeographical research \cite{bahls2014new, drury2019continent}. Still, estimating trends with citizen science data is best done with data from structured projects (i.e., less biases to account for) \cite{fox2011new}, but unstructured and semistructured projects are increasingly harnessed for trend detection \cite{walker2017using, kery2009trend, kery2010site, horns2018using, van2013occupancy, pagel2014quantifying}.

The number of ecological and environmental monitoring citizen science projects is increasing \cite{pocock2017diversity, theobald2015global}, highlighting the potential that citizen science holds for the future of ecology, conservation, and natural resource management \cite{pocock2018vision, silvertown2009new, soroye2018opportunistic, mckinley2017citizen}. But a major obstacle in the future use of citizen science data remains understanding how to best extract information from 'noisy' citizen science datasets \cite{parrish2018exposing}.

However, these 'big-data' statistical approaches \cite{kelling2015taking} are most applicable for data originating from large, successful citizen science projects. What about projects that are just starting? Or projects focusing on taxa that are less popular with the general public \cite{mair2016explaining, ward2014understanding}? Are there optimal strategies for sampling in space and time, which are best able to contribute to estimating biodiversity trends?

Here, we investigate these questions with a specific objective: investigate how spatial and temporal sampling influences trend detection of biodiversity. Our approach is site-specific and dynamic: we are interested in the parameters that influence the marginal value of a given citizen science observation for a particular day. Here, we summarize the parameters in space and time which we hyptohesized would influence the value of a citizen science observation.

\begin{itemize}
  \item \textbf{Whether a site was sampled}: if a site had been previously sampled or not. We predicted that unsampled sites would be marginally more valuable than sampled previously sampled sites.
  \item \textbf{Median sampling interval}: the median of the distribution of waiting times between samples at a site. We predicted that the median sampling interval would be positively associated with the value of a citizen science observation. I.e., observations from sites with high median waiting times would be more valuable than observations from sites with low median waiting times.
  \item \textbf{Days since last sample}: the number of days between samples at a site. We predicted that the days since the last sample would positively associate with the value of a citizen science observation.
  \item \textbf{Distance to the nearest sampled site}: the distance between the site in question and the nearest sampled site. We hypothesized that for the distance to the nearest sampled site would be positively associated with the value of a citizen science observation.
  \item \textbf{Nearest neighbor sampling interval}: the median sampling interval of the nearest neighbor. We hypothesized that this would be positively influence the value of a observation, whereby well-sampled areas (i.e., multiple sites near each other with low median sampling intervals) would have lower value observations.
  \item \textbf{Number of unique days sampled}: the total number of unique days sampled for a given site. We predicted that the total number of unique days would be positively associated with the value of an observation, whereby sites with lots of observations would receive value given the long-term data originating from them.
\end{itemize}

We illustrate our approach using a popular citizen science project --- eBird \cite{sullivan2009ebird} --- and data from the Greater Sydney Region, NSW, Australia.

\section*{Results and Discussion}
We found weak evidence that unsampled sites were more marginally valuable than unsampled sites across grid sizes (Fig. S1), noting, however, that there were no unsampled grids in the 50 km grid cell size. This suggests that stratified sampling --- an approach which aims for equal sampling among grids or sites --- [REF] may not be the most applicable approach for detecting trends using citizen science data. This also suggests that citizen scientists generally are sufficiently sampling the appropriate areas: local knowledge indicates where birders should go birding.

However, there are still parameters that are significantly important for the marginal value of a citizen science observation (PARAMETER ESTIMATES FIGURE). At all four grid sizes, days since the last sample was positively associated with the marginal value of a citizen science observation. For all but the 10 km grid size, distance to the nearest sampled grid was positively associated with the marginal value of a citizen science observation. Conversely, the results for median sampling interval were negatively associated with the value of a citizen science observation. Nearest neighbor sampling interval had relatively weak influence on the value of a citizen science observation, compared with other parameters, as did the number of unique days sampled for a given grid.

These results suggest that the value of a given citizen science observation can indeed be predicted based on temporal and spatial sampling. Through time, the longer a grid goes unsampled (i.e., days since last sample), that grid's marginal value will continue to increase. Similarly, this is synergistic with the distance to the nearest sampled grid, whereby the further away from a sampled grid an observation is, the more valuable that observation is.

We were also able to use our fitted models to predict the expected value of a given citizen science observation, dynamically, for any given day. This approach required us to look backward first, in order to predict future values. (STATIC MAP FIGURE).

We found generally consistent results, based on the grid size chosen, highlighting the influence of spatial scale of our approach. Ultimately, this approach could be used regardless of whether a citizen science project is global, regional, or carried out in the constraints of a local park; the site can be gridded and observations can then be guided into the future. It is likely that the spatial grid cells would need to change, relative to the spatial scale of the study.

Citizen science projects need to continuously adapt to be successful, and to better inform conservation science. Even if a project may not have specific objectives/goals, it is important for the project to adapt to particular questions. We envision a dynamic approach in the future of citizen science projects, which would ultimately guide participants to sites which \textit{should} be sampled on any given day. Feedback to citizen science participants is important! This approach evolves citizen science projects, moving past leaderboards which show the number of species or number of records for a participant, and show the 'value' of a participant's observations. Imagine visitor centers across the world at national parks or urban greenspaces providing users with a localized map showing which trail someone should visit if they are interested in contributing to citizen science. More broadly, this approach could be applied to anyone in the world, and citizen science participants of a given project could opt in to receive daily, weekly, or monthly updates on where they should sample. The global pull of ecotourism is increasing exponentially, allowing the the potential for people to contribute to local biodiversity knowledge in areas that are traditionally undersampled.

Here, our framework focused on a specific statistical outcome: trend detection. Many other ecological outcomes arise from citizen science, including species distribution models, phylogeographical research, invasive species detection, or phenological research. These potential outcomes will have different optimal sampling strategies in space and time. For example, an intended outcome of a spcecies distribution model would likely place greater value on observations from unsampled grids than for species trend detection. We note that our framework is robust, with the necessary piece of information being a statistical model to extract leverage.

Arguably, citizen science will continue to shape the future of ecology and conservation, as it has substantially for the past 100 years [REFS]. But we need to look forward the future: how can we maximize the immense volunteer effort? In eBird alone, citizen scientists contributed XXXX number of volunteer hours focused on looking for birds. Ultimately, birders do a good job of sampling, but we believe that we can still optimize the collective citizen science effort. We provide a framework with which citizen science project managers can use fit-to-purpose.

\matmethods{ We tested our predictions throughout the Greater Sydney Region, delineating grids across the region of varying size: 5, 10, 25, and 50 km^{2} (Fig. SX), where a grid represented a 'site'. We used the R statistical environment \cite{rcoreteam2018r} to carry out all analyses, relying heavily on the tidyverse \cite{wickham2017tidyverse}, ggplot2 \cite{wickham2016ggplot}, and sf \cite{pebesma2018sf} packages.

In order to test our predictions, we relied on the eBird basic dataset (version ebd_relDec-2018; available at: <https://ebird.org/data/download>), subsetting the data between January 1^{st}, 2010 to December 31^{st}, 2018. eBird is a particular successful citizen science project with > 600 million observations contributed by > 400 thousand participants, globally \cite{sullivan2009ebird, sullivan2014ebird, sullivan2017using}. eBird relies on volunteer birdwatchers who collect data in the form of 'checklists' --- a list of all species identified (audibly or visually) for given spatiotemporal coordinates. eBird relies on an extensive network of regional reviewers who are local experts of the avifauna \cite{gilfedder2019brokering} to ensure data quality \cite{sullivan2009ebird}.

\subsection*{Trend detection model} We first filtered the eBird basic dataset \cite{callaghan2017assessing, johnston2018estimates, la2014role}, by the following criteria: (1) we only included complete checklists, (2) only included terrestrial bird species, (3) removed any nocturnal checklists, (4) only included checklists which were > 5 minutes and < 240 minutes in duration, (5) only included checklists which travelled < 5 km or covered < 500 Ha, and (6) only included checklists which had > 4 species on it, as checklists with less than 4 species were likely to be targeted searches for particular species \cite{walker2017using, szabo2010regional}.

For any species with > 50 observations (N=235), we fit a generalized linear model using the 'glm' function in R, based on presence/absence \cite{walker2017using, horns2018using}. The models consisted of a continuous term for day, beginning on January 1^{st}, 2010, and a categorical term for county, providing a spatial component to the models. We also included an offset term for the number of species seen on a given eBird checklist, accounting for temporal and spatial effort of that checklist \cite{szaboe2010regional}. Each of the models were fit with a binomial family distribution. A total of 25,995 observations (i.e., eBird checklists) were used to fit each of the models.

\subsection*{Statistical leverage} Statistical leverage measures the influence of a particular observation on the independent variable \cite{cook1977detection}. In other words, it is a measure of how much a given observation influences the results of a statistical outcome. In our instance, we had multiple predictor variables in our GLMs, and so we used dfBeta as a measure of statistical leverage for each observation. dfBeta measures the change to the observed parameters of a new regression equation, after omitting the \textit{i^{th}} observation from the dataset \cite{belsley1980regression}. It follows the formula:

\begin{equation}
D F B E T A=\hat{\beta}-\beta_{(i)}=\frac{\left(X^{\prime} X\right)^{-1} X_{i} r_{i}}{1-h_{i}}
\end{equation}

where X is th predictor variable matrix, \textit{r} the residual vector, \textit{i h} the \textit{i^{th}} diagonal member, and \textit{i x} the \textit{i^{th}} line of matrix X. The value of dfBeta proportionally decreases with increasing number of observations.

In our case, each observation for a given species recieved a dfBeta value (i.e., each species received 25,995 measures of dfBeta), using the 'dfbetas' function from R \cite{rcoreteam2018r}. The measure of statistical leverage, then, of a given checklist was the sum of the absolute value of the dfBeta measures for each species (i.e., the sum of all 235 dfBetas). This measure of statistical leverage was thus a measure of a checklist's influence in understanding cumulative species' trends throughout the Greater Sydney Region.

\subsection*{Parameter calculation} After our model was fit from 2010---2018, we calculated the predicted parameters of interest for each day in 2018 (N=365). For each individual grid, at each of the grid sizes, we dynamically calculated the following parameters: (1) whether a grid cell had ever been sampled, (2) the distance to the nearest sampled grid cell, (3) the median sampling interval of a grid cell, (4) the median sampling interval of the nearest sampled grid cell, (5) days since the last sample in a grid cell, and (6) the duration of sampling in a grid cell --- most recent sample minus the earliest sampled date, and (7) the number of unique sampling days within the grid cell.

We then subsetted the leverage calculations (see above) for each of the days in 2018, given we know where people sampled, relative to the parameters for each of the grids on that day. We ran a linear regression, for each of the different grid sizes considered in the analysis, to investigate what parameters were of significant interest. Prior to modelling, duration was highly correlated with median sampling interval for the majority of the grid size analyses, and as such, was excluded from consdieration. Given the paramters' correlation varies among grid cell sizes, we need to ensure a robust, and simple model. All variables were standardized prior to modelling, ensuring that the effect sizes of the given parameters were meaningful. The response variable, dfBeta (i.e., value) was log-transformed prior to modelling to meet model assumptions. Thus, the final model included a log-tranformed dfBeta response variable, regressed against standardized median sampling interval, number of days sampled, days since the last sample, distance to the nearest sampled neighbor, and the neighbor's median sampling interval.

After our model was fit, we used the 'augment' function from the broom package \cite{robinson2018broom} to predict the expected leverage for every grid cell in the Greater Sydney Region, for every day. For grid cells which were unsampled, we assigned them the mean of the sampled grid cells, based on our lack of evidence that unsampled cells were significantly more valuable than sampled cells. This prediction was done for every day of 2018.

\subsection*{Data availability} eBird data are available for download, but the necessary portion of the dataset, along with spatial data, and code to reproduce our analyses are available at: GitHub repository.
}

\showmatmethods{} % Display the Materials and Methods section

\acknow{We thank the countless citizen scientists who are contributing data that is continuously increasing our collective knowledge of biodiversity.}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{refs}

\end{document}
